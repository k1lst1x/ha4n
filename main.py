import logging
import config as cfg
import murkups as nav
import requests

from aiogram import Bot, Dispatcher, executor, types
from aiogram.types.message import ContentTypes, Message
from background import keep_alive
from urllib.parse import urlencode
from bs4 import BeautifulSoup
from urllib.parse import urljoin

API_TOKEN = cfg.TOKEN

# Configure logging
logging.basicConfig(level=logging.INFO)

# Initialize bot and dispatcher
bot = Bot(token=API_TOKEN)
dp = Dispatcher(bot)

def extract_links_from_section(section, base_url):
  soup = BeautifulSoup(str(section), 'html.parser')
  links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)]
  return links

def extract_section(url, section_class):
  response = requests.get(url)

  if response.status_code == 200:
      soup = BeautifulSoup(response.text, 'html.parser')

      section_content = soup.find('section', class_=section_class)

      if section_content:
          links = extract_links_from_section(section_content, url)
          return '\n\n'.join(links[:17])
      else:
          return f"–°–µ–∫—Ü–∏—è —Å –∫–ª–∞—Å—Å–æ–º '{section_class}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ."
  else:
      return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ö–æ–¥ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {response.status_code}"

def extract_links_from_ul(ul, base_url):
  soup = BeautifulSoup(str(ul), 'html.parser')
  links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)]
  return links

def extract_ul(url, ul_class):
  response = requests.get(url)

  if response.status_code == 200:
      soup = BeautifulSoup(response.text, 'html.parser')

      ul_content = soup.find('ul', class_=ul_class)

      if ul_content:
          links = extract_links_from_ul(ul_content, url)
          return '\n\n'.join(links[:15])
      else:
          return f"UL —Å –∫–ª–∞—Å—Å–æ–º '{ul_class}' –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ."
  else:
      return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ö–æ–¥ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {response.status_code}"

@dp.message_handler(commands=['start'])
async def send_welcome(message: Message):
  if message.chat.type == 'private':
    await message.answer(
        "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –≠—Ç–æ—Ç –±–æ—Ç —É–º–µ–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ –Ω–∏–∫–Ω–µ–π–ºy –∏–ª–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏. üïµÔ∏è‚Äç‚ôÇÔ∏è", reply_markup=nav.BotMenu)

@dp.message_handler(content_types=types.ContentTypes.TEXT)
async def process_username(message: types.Message):
  if message.chat.type == 'private':
      username = message.text

      url = f"https://yandex.ru/search/?text=%22{username}%22"
      ul_class = 'serp-list'

      result = extract_ul(url, ul_class)

      await message.reply(result)

@dp.message_handler(content_types=ContentTypes.PHOTO)
async def what_photo(message: Message):
  photo = message.photo[-1]

  file_info = await bot.get_file(photo.file_id)

  photo_url = f"https://api.telegram.org/file/bot{API_TOKEN}/{file_info.file_path}"

  yandex_search_url = 'https://yandex.ru/images/search'
  yandex_payload = {'source': 'collections', 'rpt': 'imageview', 'url': photo_url}

  url = f"{yandex_search_url}?{urlencode(yandex_payload)}"
  section_class = 'CbirSites'
  result = extract_section(url, section_class)
  await message.answer(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ:\n\n{result}")

@dp.callback_query_handler(text="srch")
async def srch(callback: types.CallbackQuery):
  await bot.delete_message(callback.from_user.id, callback.message.message_id)
  await bot.send_message(callback.from_user.id, "–í–≤–µ–¥–∏—Ç–µ –Ω–∏–∫–Ω–µ–π–º –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é:")

keep_alive()

if __name__ == '__main__':
  executor.start_polling(dp, skip_updates=True)
